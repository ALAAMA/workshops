# -*- eval: (save-excursion (org-babel-goto-named-src-block "workshopPreamble") (org-babel-execute-src-block)) -*-
#+TITLE:     RCE Batch Processing Examples
#+AUTHOR:    Ista Zahn
#+EMAIL:     istazahn@gmail.com
#+DATE:      

#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="http://tutorials.iq.harvard.edu/org-html-themes/styles/readtheorg/css/readtheorg.css"/>

#+HTML_HEAD: <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
#+HTML_HEAD: <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://tutorials.iq.harvard.edu/org-html-themes/styles/readtheorg/js/readtheorg.js"></script>


#+PROPERTY: cache no
#+PROPERTY: results output
#+PROPERTY: exports both
#+PROPERTY: session nil
#+PROPERTY: comments no
#+PROPERTY: header-args:R  :session *R*
#+PROPERTY: header-args:python  :results output :session *Python*

#+name: workshopPreamble
#+begin_src emacs-lisp :exports none :results silent :tangle no
  ;; default image width of 600
  (setq-local org-image-actual-width 600)

  ;; no subscripts
  (setq-local org-export-with-sub-superscripts '{})

  ;; present all output in blocks
  (setq-local org-babel-min-lines-for-block-output 0)

  ;; no sub-section numbers
  (setq-local org-export-with-section-numbers 1)

  ;; do not re-evaluate source code on export
  (setq-local org-export-babel-evaluate nil)

  ;; enable source code support in orgmode
  (org-babel-do-load-languages
   'org-babel-load-languages
   '(;(stata . t) ;; requires custom ob-stata.el
     (emacs-lisp . t)
     (sh . t)
     (R . t)
     (latex . t)
     (octave . t)
     (ditaa . t)
     (org . t)
     (perl . t)
     (python . t)
     (matlab . t)))

  ;; display images in the orgmode buffer automatically
  (add-hook 'org-babel-after-execute-hook 'org-display-inline-images)

  (setq-local org-html-postamble
              "<h4><a href='../RCEintro.html'> Back to RCE Quick-Start Guide</a></h4>")

#+end_src

* R Batch Processing Examples

** Batch example: Simple power simulation in R
The simplest kind of batch job is one for which you just want to run the same code multiple times, without varying any parameters. For example, suppose that we wish to run a power simulation for a t.test with unequal group sizes. 

*** R power simulation script
The first step is to write a script or program to carry out the desired computation. The R script below simulates distributions with a specified mean difference, performs two-sample t-tests on the difference, and calculates the proportion of significant tests.
#+BEGIN_SRC R :tangle R_examples/power1/power.R 
  ## function to simulate data and perform a t.test
  sim.ttest <- function(mu1, mu2, sd, n1, n2) {
      d <- data.frame(x = c(rep("group1", n1), rep("group2", n2)),
                      y = c(rnorm(n1, mean = mu1, sd = sd),
                            rnorm(n2, mean = mu2, sd = sd)))
      return(t.test(y ~ x, data = d)$p.value)
  }

  ##  run the function 10,000 times 
  p <- replicate(10000,
                 sim.ttest(mu1 = 1,
                           mu2 = 1.3,
                           sd = 1,
                           n1 = 50,
                           n2 = 150))
  ## calculate the proportion of significant tests
  cat(length(p[p < .05])/length(p))
#+END_SRC

#+RESULTS:

*** Submit file
If we want to run this function one million times it may take a while, especially if our computer is an older less powerful model. So let's run it on 100 separate machines (each one will simulate the test 10000 times). To do that we need, in addition to the R script above, a submit file to request resources and run the computation. 
#+BEGIN_SRC conf :eval no :tangle R_examples/power1/power.submit
  # Universe whould always be 'vanilla'. This line MUST be 
  #included in your submit file, exactly as shown below.
  Universe = vanilla

  # Enter the path to the R program.
  Executable = /usr/local/bin/R

  # Specify any arguments you want to pass to the executable
  # to make r not save or restore workspaces, and to 
  # run as quietly as possible
  Arguments = --no-save --no-restore --slave

  # Specify the relative path to the input file
  input = power.R

  # Specify where to output any results printed by your program.
  output = output/out.$(Process)
  # Specify where to save any errors returned by your program.
  error = output/error.$(Process)
  # Specify where to save the log file.
  Log = output/log
  # Enter the number of processes to request.
  Queue 100
#+END_SRC

Now that we have our script and the submit file we can run submit the job as follows:
1. make a project folder for this run if it doesn't exist
2. save the R script (as power.R) and the submit file (as power.submit) in the project folder
3. make a sub folder named =output=
4. open a terminal and =cd= to the project folder
5. run =condor_submit power.submit= to submit the jobs to the cluster

#+BEGIN_SRC sh :exports none :results silent
  cd examples
  zip -r power1 power1
#+END_SRC

*** Monitoring submitted jobs
After submitting the jobs we may wish to monitor them, e.g. to check if they are running. You can do this by running =condor_q <your_user_name>= in a terminal. If this returns nothing then you have no jobs in the queue. Otherwise you will see information for each request in the queue which will look something like this:
#+BEGIN_EXAMPLE
  -- Schedd: HMDC.batch@rce6-5.hmdc.harvard.edu : <10.0.0.10:9619?sock=7858_e19e_247>
   ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   200.0   izahn           4/27 11:45   0+00:00:04 R  0   0.0  R --no-save --no-r
   200.1   izahn           4/27 11:45   0+00:00:04 R  0   0.0  R --no-save --no-r
   200.2   izahn           4/27 11:45   0+00:00:04 R  0   0.0  R --no-save --no-r
   200.3   izahn           4/27 11:45   0+00:00:04 R  0   0.0  R --no-save --no-r
   200.4   izahn           4/27 11:45   0+00:00:04 R  0   0.0  R --no-save --no-r
   200.5   izahn           4/27 11:45   0+00:00:04 R  0   0.0  R --no-save --no-r
   200.6   izahn           4/27 11:45   0+00:00:04 R  0   0.0  R --no-save --no-r
   200.7   izahn           4/27 11:45   0+00:00:04 R  0   0.0  R --no-save --no-r
   200.8   izahn           4/27 11:45   0+00:00:04 R  0   0.0  R --no-save --no-r
   200.9   izahn           4/27 11:45   0+00:00:04 R  0   0.0  R --no-save --no-r
   200.10  izahn           4/27 11:45   0+00:00:04 R  0   0.0  R --no-save --no-r
   200.11  izahn           4/27 11:45   0+00:00:04 R  0   0.0  R --no-save --no-r
   200.12  izahn           4/27 11:45   0+00:00:04 R  0   0.0  R --no-save --no-r
#+END_EXAMPLE
Perhaps the most important information returned by =condor_q= is the program status (the *ST* column). Status *I* means your job is in the queue but has not yet started running, *R* means the job is currently running, and *H* means the job is on hold. If you job is on hold you can get more information about what the problem might be by running =condor_q -hold=.

You will know your job is finished when it is no longer listed in the =condor_q= output. When it finishes you can examine the output and/or error files to see if the program exited successfully.

*** Aggregating results
When your batch job is finished you are usually left with multiple output files that need to be aggregated. In the case of our simulation example, we have files =output/out.0 -- output/out40=, each of which contains a single number representing the proportion of significant tests. We can aggregate them with a simple R script, like this:
#+BEGIN_SRC R :eval no :tangle R_examples/power1/aggregate.R
  ## list all output files in the output directory
  output_files <- list.files("output",
                             pattern = "^out\\.[0-9]+$",
                             full.names=TRUE)

  ## read each file, convert it to a number, and take the average
  mean(as.double(sapply(
                        output_files,
                        readLines,
                        warn = FALSE)))
#+END_SRC

*** Try it yourself! 
Download the [[file:R_examples/power1.zip][power simulation example files]], to the RCE, extract the zip file and running =condor_submit power.submit= in the =power1= directory.

** Batch example: Power simulation in R with varying parameters
The previous example was relatively simple, because we wanted to run exactly the same code on all 100 nodes. Often however you want each node to do something slightly different. For example, we may wish to vary the sample size from 100 -- 500 in increments of 10, to see how power changes as a function of that parameter. In that case we need to pass some additional information to each process, telling it which parameter space it is responsible for. 

As it turns out, we almost already know how to do that: if you you look closely at the submit file in the previous example you will notice that we used =$(Process)= to append the process number to the output and error files. 
*** Submit file passing process as an argument
We can use the =$(Process)= macro to pass information to our program, like this:
#+BEGIN_SRC conf :eval no :tangle R_examples/power2/power.submit
  # Universe whould always be 'vanilla'. This line MUST be 
  #included in your submit file, exactly as shown below.
  Universe = vanilla

  # Enter the path to the R program.
  Executable = /usr/local/bin/R

  # Specify any arguments you want to pass to the executable
  # to make r not save or restore workspaces, and to 
  # run as quietly as possible
  Arguments = --no-save --no-restore --slave --args $(Process)

  # Specify the relative path to the input file
  input = power.R

  # Specify where to save any errors returned by your program.
  error = output/error.$(Process)

  Log = log.txt
  # Enter the number of processes to request.
  Queue 40
#+END_SRC
Notice that we used =--args $(Process)= to pass the process number to the R program. =$(Process)= will be an integer starting from =0=. 

*** R script argument processing
Next we need to 1) retrieve the process number in our R program and 2) map it to the parameter space. We can retrieve the arguments in R like this:
#+BEGIN_SRC R :eval no :tangle R_examples/power2/power.R
  ## retrieve arguments passed from the command line.
  process <- as.integer(as.character(commandArgs(trailingOnly = TRUE)))
#+END_SRC
We now have a variable in R that tells us which process we are. Now we need to map that to our parameter space; recall that we want to test sample sizes from 100 to 500, so we need to map =process 0= to =n = 100=,  =process 1= to =n = 110=, =process 2= to =n = 120= and so on:
#+BEGIN_SRC R :eval no :tangle R_examples/power2/power.R
  ## map process to sample size parameter.
  n <- (process + 100) + (process*10 - process)
#+END_SRC

#+RESULTS:
#+begin_example
Error: object 'process' not found
#+end_example

There is one additional complication we need to handle: in the previous example we did need to keep track of the parameters used by each process because the parameters did not vary. Now that they do, it would be nice if we had output that recorded the value of the varying parameter as well as the result. We could of course just print the =n= parameter we calculated from the process number along with the result, but it will be easier to combine the outputs if we write them to a machine-readable format (e.g., a comma-separated-values file). You may have noticed that in the submit file above I omitted the =output= directive: that is because we are going to explicitly save the results in the R script, so we don't need the batch scheduler to save those output files for us.

Now we can set up the simulation as before, passing the =n= calculated above into our simulation function, writing the results to files. 
#+BEGIN_SRC R :eval no :tangle R_examples/power2/power.R
    ## function to simulate data and perform a t.test
    sim.ttest <- function(mu1, mu2, sd, n1, n2) {
        d <- data.frame(x = c(rep("group1", n1), rep("group2", n2)),
                        y = c(rnorm(n1, mean = mu1, sd = sd),
                              rnorm(n2, mean = mu2, sd = sd)))
        return(t.test(y ~ x, data = d)$p.value)
    }

    ##  run the function 10,000 times 
    p <- replicate(10000,
                   sim.ttest(mu1 = 1,
                             mu2 = 1.3,
                             sd = 1,
                             n1 = n,
                             n2 = n))
  write.csv(data.frame(n = n, power = length(p[p < .05])/length(p)),
            row.names = FALSE,
            file = paste0("output/out", process, ".csv"))
#+END_SRC

#+BEGIN_SRC sh :exports none :results silent
  cd examples
  zip -r power2 power2
#+END_SRC


Now we have all the required elements to submit out job, and can do so using =condor_submit= as before. 

*** Aggregating results
Each of our 40 processes produced a file in the =output= directory name =out<process>csv=; our next task is to aggregate these results. The R script below reads each of these files, joins them together into a single data.frame, and plots the result.
#+BEGIN_SRC R :eval no :tangle R_examples/power2/aggregate.R
  ## list all output files in the output directory
  output_files <- list.files("output",
                             pattern = "^out[0-9]+\\.csv$",
                             full.names=TRUE)

  ## read each file and append them
  results <- do.call(rbind, lapply(output_files, read.csv))

  ## plot
  plot(results)
  abline(h = 0.8)
#+END_SRC

[[file:images/powerDist.png]]

*** Try it yourself! 
Download the [[file:R_examples/power2.zip][power simulation example files]], to the RCE, extract the zip file, and run the example by calling =condor_submit power.submit= from the =power2= directory.

* TODO Stata Batch Processing Examples

* TODO Python Batch Processing Examples

* TODO Matlab Batch Processing Examples
